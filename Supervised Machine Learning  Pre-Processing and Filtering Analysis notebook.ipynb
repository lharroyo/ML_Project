{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvH9DIc7JepH"
   },
   "source": [
    "### References\n",
    "\n",
    "- https://www.openml.org/apis open ml documentation\n",
    "- https://www.youtube.com/watch?v=D6saJ9R65L4 imputation with pandas\n",
    "- https://www.youtube.com/watch?v=fUuvN9iuaNE one hot encoder\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html one hot encoder\n",
    "- https://www.cienciadedatos.net/documentos/py06_machine_learning_python_scikitlearn.html preprocessing\n",
    "- http://www.caprinomics.com/projects/madelon-madness/ Feature Selection Using f_classif\n",
    "- https://jdvelasq.github.io/courses/notebooks/sklearn_supervised_01_feature_selection/1-07_SelectPercentile.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dedp9wN9JepR"
   },
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1664507086714,
     "user": {
      "displayName": "Viviana Reina Díaz",
      "userId": "13947789332745390454"
     },
     "user_tz": 300
    },
    "id": "rQN8Ggv3JepT"
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import impute, tree, pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import re\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2,f_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.impute import KNNImputer\n",
    "import sklearn.datasets\n",
    "import openml\n",
    "import random\n",
    "from openml.datasets.functions import create_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cA01EKJJepW"
   },
   "source": [
    "### **Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLARNl_yJepa"
   },
   "source": [
    "On this sheet, we are working with the datasets mushroom, madelon, and amazon-commerce-reviews from openml.org. You can use the API from openml.org to download these datasets (ids 24, 1485, and 1457)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 14584,
     "status": "ok",
     "timestamp": 1664507101285,
     "user": {
      "displayName": "Viviana Reina Díaz",
      "userId": "13947789332745390454"
     },
     "user_tz": 300
    },
    "id": "L-i1E6gaJepd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1485 from '/home/felix/.cache/openml/org/openml/www/datasets/1485/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1457 from '/home/felix/.cache/openml/org/openml/www/datasets/1457/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    }
   ],
   "source": [
    "# List all datasets and their properties\n",
    "openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "\n",
    "# Get dataset by ID\n",
    "mushroom = openml.datasets.get_dataset(24)\n",
    "madelon = openml.datasets.get_dataset(1485)\n",
    "amazon = openml.datasets.get_dataset(1457)\n",
    "\n",
    "# Get dataset by name\n",
    "# mushroom = openml.datasets.get_dataset('mushroom')\n",
    "dataset_madelon = openml.datasets.get_dataset('madelon')\n",
    "dataset_amazon  = openml.datasets.get_dataset('amazon-commerce-reviews')\n",
    "\n",
    "\n",
    "# Get the data itself as a dataframe (or otherwise)\n",
    "X, y, z, w = mushroom.get_data(dataset_format=\"dataframe\")\n",
    "df_mad, y, _, _ = dataset_madelon.get_data(dataset_format=\"dataframe\")\n",
    "df_amazon, y, _, _ = dataset_amazon.get_data(dataset_format=\"dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8K0euqLJeqC"
   },
   "source": [
    "**1.** write a function get_best_encoding(df, label, learner, scoring), where df is a pandas dataframe and label is the target column.The function should look at combinations of:\n",
    "\n",
    "- dropping or keeping a column for the first value in each category\n",
    "- every technique of imputation (if the dataset has missing data).\n",
    "\n",
    "For each combination, mount a pipeline with the respective pre-processing steps and learner, and check the score of a cross-validation given the scoring function. Return the pair X, y, where X is the encoding for which the best result was obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código basado en Ciencia de datos.net\n",
    "\n",
    "def get_column_types (df):\n",
    "    A = df.select_dtypes(include =[\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]).columns.to_list() #numeric attributes\n",
    "    B = df.select_dtypes(include =[\"object\",\"category\"]).columns.to_list() #categorical attributes\n",
    "    return A, B\n",
    "\n",
    "def get_best_encoding(df, label,learner,scoring):\n",
    "    X = df.drop('class', axis = 1)\n",
    "    y = df[label]\n",
    "    \n",
    "    A,B=get_column_types (X)\n",
    "    \n",
    "    numeric_transformer_median = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "    numeric_transformer_mean = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean'))])\n",
    "    numeric_transformer_KNN = Pipeline(steps=[('imputer',KNNImputer(n_neighbors=5, weights='uniform',\n",
    "                                                                    metric='nan_euclidean'))])\n",
    "# Transformaciones para las variables categóricas\n",
    "    transformer_categorical_colls = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                              ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    transformer_categorical_drop_if_binary = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                             ('onehot', OneHotEncoder(handle_unknown='ignore',drop='if_binary'))])    \n",
    "    transformer_categorical_drop_first = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]) \n",
    "    \n",
    "    preprocessor_median_categoric = ColumnTransformer(transformers=[('numeric_median', numeric_transformer_median, A),\n",
    "                                                   ('categoric', transformer_categorical_colls, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')\n",
    "    preprocessor_median_categoric_ifbinary = ColumnTransformer(transformers=[('numeric_median', numeric_transformer_median, A),\n",
    "                                                   ('categoric', transformer_categorical_drop_if_binary, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')    \n",
    "    preprocessor_median_categoric_first = ColumnTransformer(transformers=[('numeric_median', numeric_transformer_median, A),\n",
    "                                                   ('categoric', transformer_categorical_drop_first, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')\n",
    "    preprocessor_mean_categoric = ColumnTransformer(transformers=[('numeric_mean', numeric_transformer_mean, A),\n",
    "                                                   ('categoric', transformer_categorical_colls, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')    \n",
    "    preprocessor_mean_categoric_ifbinary = ColumnTransformer(transformers=[('numeric_mean', numeric_transformer_mean, A),\n",
    "                                                   ('categoric', transformer_categorical_drop_if_binary, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')       \n",
    "    preprocessor_mean_categoric_first = ColumnTransformer(transformers=[('numeric_mean', numeric_transformer_mean, A),\n",
    "                                                   ('categoric', transformer_categorical_drop_first, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')      \n",
    "    preprocessor_Knn_categoric = ColumnTransformer(transformers=[('numeric_Knn', numeric_transformer_KNN, A),\n",
    "                                                   ('categoric', transformer_categorical_colls, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')    \n",
    "    preprocessor_Knn_categoric_ifbinary = ColumnTransformer(transformers=[('numeric_Knn', numeric_transformer_KNN, A),\n",
    "                                                   ('categoric', transformer_categorical_drop_if_binary, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')       \n",
    "    preprocessor_Knn_categoric_first = ColumnTransformer(transformers=[('numeric_Knn', numeric_transformer_KNN, A),\n",
    "                                                   ('categoric', transformer_categorical_drop_first, B)\n",
    "                                                  ],                \n",
    "                                     remainder='passthrough')    \n",
    "    \n",
    "    list =[preprocessor_median_categoric,preprocessor_median_categoric_ifbinary,preprocessor_median_categoric_first,\n",
    "          preprocessor_mean_categoric,preprocessor_mean_categoric_ifbinary,preprocessor_mean_categoric_first,\n",
    "          preprocessor_Knn_categoric,preprocessor_Knn_categoric_ifbinary,preprocessor_Knn_categoric_first]\n",
    "    \n",
    "    X_analysis, X_test, y_analysis, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    i=0\n",
    "    names=[\"median\", \"medianifbinary\", \"medianoutfirst\", \"mean\", \"meanifbinary\", \"meanoutfirst\", \"knn\", \"knnifbinary\", \"knnoutfirst\"]\n",
    "    for preprocessor in list:\n",
    "        \n",
    "        X_train_data = preprocessor.fit_transform(X_analysis)\n",
    "        X_test_data = preprocessor.transform(X_test)\n",
    "        encoded_categoric = preprocessor.named_transformers_['categoric']['onehot']\\\n",
    "              .get_feature_names_out(B)\n",
    "        labels = np.concatenate([A, encoded_categoric])\n",
    "        pl = Pipeline([(\"Preprocessor\", preprocessor), (\"classifier\", learner)])\n",
    "        Score_data = cross_val_score(estimator=pl,X=X,y=y,scoring=scoring)\n",
    "        print(f\"{names[i]}, cv_scores mean: \",Score_data.mean())\n",
    "        i+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyg4lWcJJeqs"
   },
   "source": [
    "**2.** Apply the function to the mushroom dataset once for kNN and once for a decision tree. Look at the class distribution and choose an appropriate scoring function (justify your choice!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1664507101581,
     "user": {
      "displayName": "Viviana Reina Díaz",
      "userId": "13947789332745390454"
     },
     "user_tz": 300
    },
    "id": "VTSQpv3FJeqA",
    "outputId": "c2abdf6a-3614-4c1e-e6d2-6bdfb8033ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Knn\n",
      "median, cv_scores mean:  0.8984180371352786\n",
      "medianifbinary, cv_scores mean:  0.9039584691170898\n",
      "medianoutfirst, cv_scores mean:  0.917505115574081\n",
      "mean, cv_scores mean:  0.8984180371352786\n",
      "meanifbinary, cv_scores mean:  0.9039584691170898\n",
      "meanoutfirst, cv_scores mean:  0.917505115574081\n",
      "knn, cv_scores mean:  0.8984180371352786\n",
      "knnifbinary, cv_scores mean:  0.9039584691170898\n",
      "knnoutfirst, cv_scores mean:  0.917505115574081\n",
      "For DT\n",
      "median, cv_scores mean:  0.9647981811292157\n",
      "medianifbinary, cv_scores mean:  0.9647981811292157\n",
      "medianoutfirst, cv_scores mean:  0.9812904888215233\n",
      "mean, cv_scores mean:  0.9647981811292157\n",
      "meanifbinary, cv_scores mean:  0.9646751042061388\n",
      "meanoutfirst, cv_scores mean:  0.9133198938992043\n",
      "knn, cv_scores mean:  0.9647981811292157\n",
      "knnifbinary, cv_scores mean:  0.9232957938613111\n",
      "knnoutfirst, cv_scores mean:  0.9290737400530504\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "Knn = KNeighborsClassifier()\n",
    "print(\"For Knn\")\n",
    "get_best_encoding(X,\"class\",Knn,\"accuracy\")\n",
    "print(\"For DT\")\n",
    "get_best_encoding(X,\"class\",dt,\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Applying the function \"get_best_encoding(df, label, learner, scoring)\" to the \"mushroom\" database of the Open Dataset API, we can observe that the chosen score \"Accuracy\" is better under the KNN imputation technique, due to its functionality in the different types of data. One of the advantages applied to this type of data is that this method gives a relationship according to the variables associated with the missing information and in this case, the mushroom base only presents one column with missing values (stalk-root with 30% without information), therefore, it relates the information of the other 21 columns of information and generates a better approximation to give completeness to the data.\n",
    "\n",
    "**Porcentaje de datos faltantes de mushroom por columnas**\n",
    "\n",
    "cap-shape 0.000000 cap-surface 0.000000 cap-color 0.000000 bruises%3F 0.000000 odor 0.000000 gill-attachment 0.000000 gill-spacing 0.000000 gill-size 0.000000 gill-color 0.000000 stalk-shape 0.000000 **stalk-root 0.305268** stalk-surface-above-ring 0.000000 stalk-surface-below-ring 0.000000 stalk-color-above-ring 0.000000 stalk-color-below-ring 0.000000 veil-type 0.000000 veil-color 0.000000 ring-number 0.000000 ring-type 0.000000 spore-print-color 0.000000 population 0.000000 habitat 0.000000 dtype: float64\n",
    "\n",
    "Within this, a better measure given by the imputation is evident by eliminating the first value, due to the approximation given to this closeness value that it generates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "aborted",
     "timestamp": 1664507102672,
     "user": {
      "displayName": "Viviana Reina Díaz",
      "userId": "13947789332745390454"
     },
     "user_tz": 300
    },
    "id": "DZuIPS6TJeqt"
   },
   "source": [
    "##  Exercise 2\n",
    "\n",
    "1. Write a function analyze_filtering_benefits(X, y) in which you plug a feature selector\n",
    "(sklearn.feature_selection.SelectPercentile) together with (i) kNN and (i)\n",
    "DT into two pipelines.\n",
    "Compare the cross-validation performances of kNN/DT when filtering is used or not\n",
    "used on the given data, and report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mad = df_mad.drop('Class',axis=1)\n",
    "y_mad = df_mad['Class']\n",
    "X_amazon = df_amazon.drop('Class',axis=1)\n",
    "y_amazon = df_amazon['Class']\n",
    "\n",
    "\n",
    "\n",
    "def analyze_filtering_benefits(X, y):\n",
    "    dt = DecisionTreeClassifier()\n",
    "    Knn = KNeighborsClassifier()\n",
    "    # Filtering with the decision tree \n",
    "    pl_f_classif_Dt = Pipeline([(\"Selection\", sklearn.feature_selection.SelectPercentile(score_func=f_classif)), (\"classifier\", dt)])\n",
    "    # Filtering with KNeighbors Classifier\n",
    "    pl_Knn_f_classif = Pipeline([(\"Selection\", sklearn.feature_selection.SelectPercentile(score_func=f_classif)), (\"classifier\",Knn)])\n",
    "    # Unfiltered for the KNeighbors Classifier\n",
    "    unfiltered_Score_knn = cross_val_score(Knn,X, y)\n",
    "    # Unfiltered for the decision tree\n",
    "    unfiltered_Score_Dt = cross_val_score(dt,X,y)\n",
    "    data = {'cv scores mean filtering usign dt': [np.mean(sklearn.model_selection.cross_validate(pl_f_classif_Dt, X, y)[\"test_score\"])],\n",
    "            'cv scores mean filtering usign Knn' :[np.mean(sklearn.model_selection.cross_validate(pl_Knn_f_classif, X, y)[\"test_score\"])],\n",
    "            'cv_scores mean unfiltering dt' :[unfiltered_Score_Dt.mean()],\n",
    "            'cv_scores mean unfiltering Knn':[unfiltered_Score_knn.mean()]}\n",
    "    cv_results = pd.DataFrame(data)\n",
    "    print(cv_results)\n",
    "    \n",
    "#     print(\"Cross validation Score with filtering usign decision tree: \",(sklearn.model_selection.cross_validate(pl_f_classif_Dt, X, y)[\"test_score\"]))\n",
    "#     print(\"Cross validation Score unfiltering usign decision tree: \",unfiltered_Score_Dt)\n",
    "#     print(\"\\n\")\n",
    "#     print(\"Cross validation Score with filtering usign KNeighbors Classifier: \",(sklearn.model_selection.cross_validate(pl_Knn_f_classif, X, y)[\"test_score\"]))\n",
    "#     print(\"Cross validation Score unfiltering usign Knn: \",unfiltered_Score_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For DB Madelon\n",
      "   cv scores mean filtering usign dt  cv scores mean filtering usign Knn  \\\n",
      "0                           0.744615                            0.836154   \n",
      "\n",
      "   cv_scores mean unfiltering dt  cv_scores mean unfiltering Knn  \n",
      "0                       0.749231                           0.725  \n",
      "\n",
      " For DB Amazon-commerce-reviews\n",
      "   cv scores mean filtering usign dt  cv scores mean filtering usign Knn  \\\n",
      "0                              0.436                            0.225333   \n",
      "\n",
      "   cv_scores mean unfiltering dt  cv_scores mean unfiltering Knn  \n",
      "0                       0.400667                            0.25  \n"
     ]
    }
   ],
   "source": [
    "print(\"For DB Madelon\")\n",
    "analyze_filtering_benefits(X_mad, y_mad)\n",
    "print(\"\\n\",\"For DB Amazon-commerce-reviews\")\n",
    "analyze_filtering_benefits(X_amazon,y_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madelon data analysis\n",
    "According to the results obtained when filtering the \"Madelon\" dataset, a significant increase in the score is generated and it is more practical to use the Knn classifier than the Decision Tree, as the average result was 0.8 and higher by 0.1 compared to the other classifier.\n",
    "### Analysis of Amazon-commerce-reviews data\n",
    "According to the results when filtering is applied to the data set \"amazon-commerce-reviews\" there is a slight increase in the score and it is more practical to use the Decision Tree classifier than the Knn classifier, as the average score was 0.4 and higher by 0.2 for the other classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (Wrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De la pagina de OpenMl:\n",
    "def get_db (id):\n",
    "    dataset = openml.datasets.get_dataset(id)\n",
    "    # Print a summary\n",
    "    print(\n",
    "    f\"This is dataset '{dataset.name}', the target feature is \"\n",
    "    f\"'{dataset.default_target_attribute}'\"\n",
    "    )\n",
    "    print(f\"URL: {dataset.url}\")\n",
    "    print(dataset.description[:20])\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    "    )\n",
    "    return (X,y,categorical_indicator, attribute_names)\n",
    "def get_subdf (X, attr):\n",
    "    n= np.random.randint((len(attr)))\n",
    "    lista= np.random.choice(attr,n )\n",
    "    X_new = X[lista]\n",
    "    return (lista, X_new,n)\n",
    "def fill_row (attr,lista,score):\n",
    "    row=[]\n",
    "    for a in attr:\n",
    "        if a in lista: row.append(1)\n",
    "        else: row.append(0)\n",
    "    return (row)\n",
    "def get_score (X, y,learner, repeats, train_size, scoring):\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y, train_size = train_size)\n",
    "    if learner == \"dt\": clf = sklearn.tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
    "    elif learner == \"knn\": clf = KNeighborsClassifier().fit(X_train,y_train)\n",
    "    cv = ShuffleSplit(n_splits=repeats, train_size=train_size)\n",
    "    score= np.abs(np.mean (cross_val_score(clf, X_train, y_train, cv=cv, scoring = scoring)))\n",
    "    return (score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Write a function wrapping(learner, X, y, train_size, repeats, scoring, tries) that will randomly create tries subsets of attributes in X and evaluate the learner based on that sub-database using MCCV with the given train size and repeats, using the given scoring function. For each feature set, memorize the mean value of the scoring function.\n",
    "Return a pandas dataframe with m+1 columns, where m is the number of attributes of X. There should be one line for every tried configuation, with the observed performance in the last cell. The other columns should be filled with 0/1 depending on whether an attribute was selected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapping(learner, X, y, train_size, repeats, scoring, tries):\n",
    "    tr = list (range(tries))\n",
    "    (f,m) = X.shape\n",
    "    attr= X.columns\n",
    "    # Crea el df general\n",
    "    df = pd.DataFrame (index = tr, columns = attr )\n",
    "    \n",
    "    # genera la lista para hacer el calculo\n",
    "    score =[]\n",
    "    for intento in tr:\n",
    "        (lista, X_new,n ) = get_subdf (X,attr)\n",
    "        row = fill_row (attr, lista, score)\n",
    "        df.loc[intento] = row\n",
    "        s = get_score (X_new, y ,learner, repeats, train_size, scoring)\n",
    "        print (\"Intento \", intento,\" con\", n, \" atributos\")\n",
    "        score.append (s)\n",
    "    df.insert(m, \"performance\", score, allow_duplicates=False)\n",
    "    print (\"\\n\", df)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Write a function analyze_wrapping_benefits(X, y) in which you create an analysis/ test fold of X/y, and use (i) kNN and (i) DT with wrapping on the analysis set to find the best feature set using wrapping.\n",
    "Compare the performance of each of them\n",
    "a) on the full analysis data and the projected analysis data.\n",
    "b) on the full test data and the projected test data.\n",
    "This should give 4 numbers in total per dataset (of course the results on the test data\n",
    "must not be used for decision making), so this is not a function that you could use inside\n",
    "your analysis (except that you separated test data before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_wrapping_benefits(X, y):\n",
    "    X_analysis, X_alejo, y_analysis, y_alejo = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "    D_alejo= pd.DataFrame(X_alejo,y_alejo) # Al cuarto de san Alejo\n",
    "    train_size = 0.8    \n",
    "    scoring=\"accuracy\"\n",
    "    repeats=10\n",
    "    tries=5\n",
    "    # Para knn\n",
    "    learner = \"knn\"\n",
    "    print (learner, \"\\n Analysis \\n\")\n",
    "    df = wrapping(learner, X_analysis, y_analysis, train_size, repeats, scoring, tries)\n",
    "    print (\"Test\\n\")\n",
    "    a=df[\"performance\"].max()\n",
    "    df = wrapping(learner, X_alejo, y_alejo, train_size, repeats, scoring, tries)\n",
    "    print (\"\\n\")\n",
    "    b=df[\"performance\"].max()\n",
    "\n",
    "    # Para dt\n",
    "    learner= \"dt\"\n",
    "    print (learner, \"\\n Analysis \\n\")\n",
    "    df = wrapping(learner, X_analysis, y_analysis, train_size, repeats, scoring, tries)\n",
    "    print (\"Test\\n\")\n",
    "    c=df[\"performance\"].max()\n",
    "    df = wrapping(learner, X_alejo, y_alejo, train_size, repeats, scoring, tries)\n",
    "    print (\"\\n\")\n",
    "    d=df[\"performance\"].max()  \n",
    "    return (a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Load the datasets madelon and amazon-commerce-reviews from openml.org. You can use the API from openml.org.\n",
    "Run the wrapping analysis on both datasets. Interpret your observations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1485 from '/home/felix/.cache/openml/org/openml/www/datasets/1485/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is dataset 'madelon', the target feature is 'Class'\n",
      "URL: https://old.openml.org/data/v1/download/1590986/madelon.arff\n",
      "**Author**: Isabelle\n",
      "knn \n",
      " Analysis \n",
      "\n",
      "Intento  0  con 15  atributos\n",
      "Intento  1  con 352  atributos\n",
      "Intento  2  con 307  atributos\n",
      "Intento  3  con 89  atributos\n",
      "Intento  4  con 392  atributos\n",
      "\n",
      "   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10  ... V492 V493 V494 V495 V496 V497 V498 V499  \\\n",
      "0  0  0  0  0  0  0  0  0  0   0  ...    0    0    0    0    0    0    0    0   \n",
      "1  1  1  1  0  0  0  0  0  0   1  ...    1    1    1    1    0    1    0    1   \n",
      "2  1  0  1  0  1  0  1  1  0   0  ...    0    0    1    1    0    1    1    1   \n",
      "3  0  0  0  0  1  0  0  0  0   0  ...    1    1    0    0    0    0    0    0   \n",
      "4  1  1  1  0  0  0  0  1  1   1  ...    0    1    0    1    1    0    0    1   \n",
      "\n",
      "  V500 performance  \n",
      "0    0    0.497297  \n",
      "1    1    0.620420  \n",
      "2    1    0.653153  \n",
      "3    0    0.608108  \n",
      "4    0    0.604204  \n",
      "\n",
      "[5 rows x 501 columns]\n",
      "Test\n",
      "\n",
      "Intento  0  con 143  atributos\n",
      "Intento  1  con 106  atributos\n",
      "Intento  2  con 384  atributos\n",
      "Intento  3  con 270  atributos\n",
      "Intento  4  con 41  atributos\n",
      "\n",
      "   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10  ... V492 V493 V494 V495 V496 V497 V498 V499  \\\n",
      "0  1  0  1  0  0  0  0  1  0   1  ...    0    1    0    0    1    0    1    0   \n",
      "1  0  0  0  0  0  0  0  1  1   1  ...    1    1    1    0    0    0    0    0   \n",
      "2  1  0  0  1  1  1  0  0  1   1  ...    1    0    0    0    0    1    1    1   \n",
      "3  0  1  1  0  1  0  0  1  1   1  ...    0    0    0    0    1    0    0    1   \n",
      "4  0  0  0  0  0  0  0  0  0   0  ...    0    0    0    0    0    0    0    0   \n",
      "\n",
      "  V500 performance  \n",
      "0    0    0.621429  \n",
      "1    0    0.611905  \n",
      "2    1    0.617857  \n",
      "3    1    0.580952  \n",
      "4    0    0.565476  \n",
      "\n",
      "[5 rows x 501 columns]\n",
      "\n",
      "\n",
      "dt \n",
      " Analysis \n",
      "\n",
      "Intento  0  con 395  atributos\n",
      "Intento  1  con 398  atributos\n",
      "Intento  2  con 53  atributos\n",
      "Intento  3  con 28  atributos\n",
      "Intento  4  con 225  atributos\n",
      "\n",
      "   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10  ... V492 V493 V494 V495 V496 V497 V498 V499  \\\n",
      "0  0  0  1  0  1  1  1  0  1   1  ...    0    1    1    1    1    1    0    1   \n",
      "1  1  0  1  1  0  1  1  1  1   0  ...    0    1    1    0    0    0    0    1   \n",
      "2  0  0  0  0  0  0  1  0  0   1  ...    0    0    0    0    0    1    0    0   \n",
      "3  0  0  0  0  0  0  0  0  0   0  ...    0    0    0    0    0    0    0    0   \n",
      "4  0  0  0  0  1  0  1  1  0   0  ...    1    0    0    0    0    0    0    0   \n",
      "\n",
      "  V500 performance  \n",
      "0    1    0.669970  \n",
      "1    0    0.633033  \n",
      "2    0    0.528529  \n",
      "3    0    0.492192  \n",
      "4    1    0.650450  \n",
      "\n",
      "[5 rows x 501 columns]\n",
      "Test\n",
      "\n",
      "Intento  0  con 7  atributos\n",
      "Intento  1  con 21  atributos\n",
      "Intento  2  con 301  atributos\n",
      "Intento  3  con 114  atributos\n",
      "Intento  4  con 360  atributos\n",
      "\n",
      "   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10  ... V492 V493 V494 V495 V496 V497 V498 V499  \\\n",
      "0  0  0  0  0  0  0  0  0  0   0  ...    0    0    0    0    0    0    0    0   \n",
      "1  0  0  0  0  0  0  0  0  0   0  ...    0    0    0    0    1    0    0    0   \n",
      "2  0  1  1  1  0  0  0  1  1   1  ...    0    1    1    1    0    0    1    0   \n",
      "3  1  0  0  0  0  0  0  0  1   0  ...    0    0    0    0    0    1    0    0   \n",
      "4  1  1  0  0  1  1  1  1  1   1  ...    1    0    1    1    0    1    0    0   \n",
      "\n",
      "  V500 performance  \n",
      "0    0    0.514286  \n",
      "1    0    0.526190  \n",
      "2    1    0.565476  \n",
      "3    0    0.514286  \n",
      "4    1    0.555952  \n",
      "\n",
      "[5 rows x 501 columns]\n",
      "\n",
      "\n",
      "Analysis KNN 0.6531531531531531\n",
      "Analysis DT 0.6699699699699699\n",
      "Test KNN 0.6214285714285716\n",
      "Test DT 0.5654761904761905\n"
     ]
    }
   ],
   "source": [
    "id= 1485  # Madelon\n",
    "#id = 1457  # Amazon\n",
    "\n",
    "# Arma el dataset general\n",
    "\n",
    "(X,y,cat,attr) = get_db (id)\n",
    "(a,b,c,d) = analyze_wrapping_benefits(X, y)\n",
    "print (\"Analysis KNN\", a)\n",
    "print (\"Analysis DT\", c)\n",
    "print (\"Test KNN\", b)\n",
    "print (\"Test DT\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis for madelon: Several runs were made and the scoring analysis with KNN is similar with DT, and will depend on the attributes to be chosen. This may be because the number of attributes are not as large as that of amazon. It is also found that their results do not improve as more attributes are used, e.g., the more the attributes are used, the better:\n",
    "Analysis KNN 0.703 Analysis DT 0.75\n",
    "Analysis for Amazon: Several runs were made and it is found that it gives better results with Decision tree and when you have more attributes for analysis. Example:\n",
    "Analysis KNN 0.236979 Analysis DT 0.3682\n",
    "This attempt for example was with 6518 attributes (similar to 4410). Whereas with fewer attributes the result is generally found to be lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc8e498aedb9fe5f41b87cf4cd311ab1a5a2920d50efca8816283812d53cc870"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
